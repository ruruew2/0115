import streamlit as st
import os
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
from langchain.chains.combine_documents import create_stuff_documents_chain

# --- 1. API í‚¤ ì„¤ì • (ë³´ì•ˆìƒ ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©) ---
os.environ["OPENAI_API_KEY"] = " "

st.set_page_config(page_title="City Plan RAG", page_icon="ğŸ™ï¸")
st.title("ğŸ™ï¸ ì„œìš¸ & ë‰´ìš• ë„ì‹œê³„íš Q&A")

# --- 2. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (í”¼í´ ì—†ì´ ì§ì ‘ ë¡œë“œ) ---
@st.cache_resource
def init_rag():
    # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (ì´ ë¶€ë¶„ì´ ì •í™•í•´ì•¼ í•©ë‹ˆë‹¤!)
    files = [
        r'C:\Users\user\Desktop\0115\ì œì™¸íŒŒì¼\g\data.pdf',
        r'C:\Users\user\Desktop\0115\ì œì™¸íŒŒì¼\g\/OneNYC_2050_Strategic_Plan.pdf'
    ]
    
    docs = []
    for f in files:
        if os.path.exists(f):
            # PyMuPDFLoaderëŠ” í•œê¸€ ì¸ì‹ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤
            loader = PyMuPDFLoader(f)
            docs.extend(loader.load())
    
    if not docs:
        st.error("âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”!")
        return None

    # í…ìŠ¤íŠ¸ ë‚˜ëˆ„ê¸°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)

    # ë²¡í„° ì €ì¥ì†Œ ë§Œë“¤ê¸°
    embeddings = OpenAIEmbeddings(model='text-embedding-3-large')
    vector_store = Chroma.from_documents(
        documents=splits, 
        embedding=embeddings,
        persist_directory="./chroma_db_new" # ì•„ì˜ˆ ìƒˆë¡œìš´ DB í´ë” ì‚¬ìš©
    )
    return vector_store.as_retriever(k=3)

# ë¦¬íŠ¸ë¦¬ë²„ ë¡œë“œ
retriever = init_rag()
llm = ChatOpenAI(model="gpt-4o-mini")

# --- 3. ì²´ì¸ ì„¤ì • ---
# ì§ˆë¬¸ ë³´ì •ìš©
q_augment_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë„ˆëŠ” ì§ˆë¬¸ ë³´ì • ì „ë¬¸ AIì•¼. ëŒ€í™” ë§¥ë½ì„ ë³´ê³  ì‚¬ìš©ìì˜ ì§§ì€ ì§ˆë¬¸ì„ ê²€ìƒ‰ ê°€ëŠ¥í•œ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¿”ì¤˜."),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{query}")
])
q_augment_chain = q_augment_prompt | llm | StrOutputParser()

# ë‹µë³€ìš©
qna_prompt = ChatPromptTemplate.from_messages([
    ("system", "ì•„ë˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•´:\n\n{context}"),
    MessagesPlaceholder(variable_name="messages"),
])
document_chain = create_stuff_documents_chain(llm, qna_prompt)

# --- 4. ì±„íŒ… UI ---
if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg.type):
        st.markdown(msg.content)

if prompt := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # 1. ì§ˆë¬¸ ë³´ì •
        aug_query = q_augment_chain.invoke({
            "chat_history": st.session_state.messages[:-1],
            "query": prompt
        })
        # 2. ê²€ìƒ‰ ë° ë‹µë³€
        docs = retriever.invoke(aug_query)
        res = document_chain.invoke({"messages": st.session_state.messages, "context": docs})
        
        st.markdown(res)
        st.session_state.messages.append(AIMessage(content=res))



# import streamlit as st
# import os
# import pickle
# from langchain_community.document_loaders import PyPDFLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
# from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# from langchain_chroma import Chroma
# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.messages import HumanMessage, AIMessage
# from langchain.chains.combine_documents import create_stuff_documents_chain


# OPENAI_API_KEY = "ì—¬ê¸°ì—_API_í‚¤ë¥¼_ì…ë ¥í•˜ì„¸ìš”" # ë³´ì•ˆì„ ìœ„í•´ ì‹¤ì œ í™˜ê²½ì—ì„  st.secrets ì‚¬ìš© ê¶Œì¥
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# st.set_page_config(page_title="City Plan RAG Chatbot", page_icon="ğŸ™ï¸")
# st.title("ğŸ™ï¸ ì„œìš¸ & ë‰´ìš• ë„ì‹œê³„íš Q&A")

# # --- 2. ë¦¬ì†ŒìŠ¤ ë¡œë“œ í•¨ìˆ˜ (ìºì‹± ì²˜ë¦¬) ---
# @st.cache_resource
# def init_rag_system():
#     # PDF ë¡œë“œ ë° í”¼í´ ì €ì¥ ë¡œì§ (ê²½ë¡œëŠ” í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
#     pdf_path = 'C:/Users/user/Desktop/0115/g/data.pdf' # ì˜ˆì‹œ ê²½ë¡œ
#     pickle_path = 'data.pkl'
    
#     if os.path.exists(pickle_path):
#         with open(pickle_path, 'rb') as f:
#             data_seoul = pickle.load(f)
#     else:
#         loader = PyPDFLoader(pdf_path)
#         data_seoul = loader.load()
#         with open(pickle_path, 'wb') as f:
#             pickle.dump(data_seoul, f)

#     # í…ìŠ¤íŠ¸ ë¶„í• 
#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
#     all_splits = text_splitter.split_documents(data_seoul)

#     # ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´
#     embeddings = OpenAIEmbeddings(model='text-embedding-3-large')
#     vector_store = Chroma.from_documents(
#         documents=all_splits, 
#         embedding=embeddings,
#         persist_directory="./chroma_db_streamlit"
#     )
#     return vector_store.as_retriever(k=3)

# retriever = init_rag_system()

# # LLM ì„¤ì •
# llm = ChatOpenAI(model="gpt-4o-mini")

# # --- 3. ì²´ì¸ ìƒì„± ---
# # ì§ˆë¬¸ ë³´ì •(Augmentation) ì²´ì¸
# q_augment_prompt = ChatPromptTemplate.from_messages([
#     ("system", "ë„ˆëŠ” ì§ˆë¬¸ ë³´ì • ì „ë¬¸ AIì•¼. ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ëª…í™•í•œ ì§ˆë¬¸ìœ¼ë¡œ êµì •í•´ì¤˜."),
#     MessagesPlaceholder(variable_name="chat_history"),
#     ("human", "{query}")
# ])
# q_augment_chain = q_augment_prompt | llm | StrOutputParser()

# # ë‹µë³€ ìƒì„± ì²´ì¸
# qna_prompt = ChatPromptTemplate.from_messages([
#     ("system", "ì•„ë˜ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n{context}"),
#     MessagesPlaceholder(variable_name="messages"),
# ])
# document_chain = create_stuff_documents_chain(llm, qna_prompt)

# # --- 4. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì±„íŒ… ê¸°ë¡ ì €ì¥) ---
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # --- 5. ì±„íŒ… UI êµ¬ì„± ---
# # ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
# for message in st.session_state.messages:
#     with st.chat_message(message.type):
#         st.markdown(message.content)

# # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
#     # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í™”ë©´ í‘œì‹œ
#     st.session_state.messages.append(HumanMessage(content=prompt))
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         with st.spinner("ìƒê° ì¤‘..."):
#             # 2. ì§ˆë¬¸ ì¦ê°• (Query Augmentation)
#             augmented_query = q_augment_chain.invoke({
#                 "chat_history": st.session_state.messages[:-1],
#                 "query": prompt
#             })
            
#             # 3. ë¬¸ì„œ ê²€ìƒ‰ (Retriever)
#             docs = retriever.invoke(augmented_query)
            
#             # 4. ìµœì¢… ë‹µë³€ ìƒì„±
#             response = document_chain.invoke({
#                 "messages": st.session_state.messages,
#                 "context": docs
#             })
            
#             st.markdown(response)
#             # ì°¸ê³  ë¬¸í—Œ í‘œì‹œ (ì„ íƒ ì‚¬í•­)
#             with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
#                 for i, doc in enumerate(docs):
#                     st.write(f"**Source {i+1}:** {doc.page_content[:200]}...")

#     # 5. AI ë‹µë³€ ì €ì¥
#     st.session_state.messages.append(AIMessage(content=response))