# chatbot5.py
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, AIMessage

from langchain_classic.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableSequence, RunnableLambda

load_dotenv()
# 1. ìŠ¤íŠ¸ë¦¼ë¦¿ ì„¤ì •
st.header("ğŸ—¨ï¸ LangChain ë©€í‹°í„´ Memory ì±—ë´‡")
st.subheader("PromptTemplate +BufferMemory")
st.caption("ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” ë©€í‹°í„´ ì±—ë´‡")

# 2. session_stateì— ë©”ëª¨ë¦¬ ì €ì¥
if "memory" not in st.session_state:
    st.session_state["memory"]=ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )
memory = st.session_state['memory']

# 3. PromptTemplate ì •ì˜
template="""
ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AIì•¼.
ì´ì „ ëŒ€í™” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ì•„:

{chat_history}
---
ì‚¬ìš©ì ì§ˆë¬¸ : {user_input}
AI ë‹µë³€ :
"""
prompt = PromptTemplate(input_variables=["chat_history","user_input"],template=template)

# 4. ëª¨ë¸ ìƒì„±
llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# 5. run_chain()í•¨ìˆ˜ êµ¬í˜„
def run_chain(inputs):
    """LLMí˜¸ì¶œ->ë©”ëª¨ë¦¬ ì €ì¥->ê²°ê³¼ ë°˜í™˜"""
    # [1] ë©”ëª¨ë¦¬ì—ì„œ ê¸°ì¡´ ëŒ€í™” ê°€ì ¸ì˜¤ê¸°
    history = memory.load_memory_variables({})["chat_history"]

    # [2] í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_text = prompt.format(chat_history=history, user_input=inputs["user_input"])

    # [3] ëª¨ë¸ í˜¸ì¶œ
    result  = llm.invoke(prompt_text)
    answer = result.content # ëª¨ë¸ ë‹µë³€

    # [4] ë©”ëª¨ë¦¬ì— ì €ì¥
    memory.save_context({"input":inputs["user_input"]},{"output": answer})
    return {"text":answer}

# 6. í•¨ìˆ˜ í˜¸ì¶œ
chain = RunnableLambda(run_chain)
# ê°ì²´ ì²´ì¸ìœ¼ë¡œ ì—°ê²°: prompt |llm|str_parser
# ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ì²´ì¸ì˜ ì¼ë¶€ë¡œ ë„£ê³  ì‹¶ë‹¤ë©´=>RunnableLambdaë¥¼ ì‚¬ìš©í•œë‹¤
# ë³€ìˆ˜=RunnableLambda(í•¨ìˆ˜)
# ë³€ìˆ˜ | íŒŒì„œ

# 7. UIì¶œë ¥ (ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ì¶œë ¥)
st.subheader("ëŒ€í™” ë‚´ìš©")
# messages = memory['chat_history'].messages # ==>ì—ëŸ¬
###################################
messages = memory.chat_memory.messages
print(memory)
###################################
# ConversationBufferMemory ì†ì„±
# memory_key="chat_history",  => í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬ë  ë³€ìˆ˜ ì´ë¦„
# chat_memory =>(ChatMessageHistoryíƒ€ì…. ì´ ì•ˆì— messagesì†ì„±ì´ ìˆë‹¤)=> ì‹¤ì œ ëŒ€í™”ë¥¼ ì €ì¥í•˜ëŠ” ë‚´ë¶€ ì €ì¥ì†Œ

for msg in messages:
    role = "user" if isinstance(msg,HumanMessage) else "assistant"
    with st.chat_message(role):
        st.write(msg.content)

# 8. ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ë©”ì‹œì§€ ì…ë ¥í•˜ì„¸ìš”")

if user_input:
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
    st.chat_message("user").write(user_input)

    # llmí˜¸ì¶œ
    response = chain.invoke({"user_input": user_input})

    # aië©”ì‹œì§€ ì¶œë ¥
    answer = response['text']
    with st.chat_message("assistant"):
        st.write(answer)
    st.rerun()

# streamlit run chatbot5.py