import os
from langchain_community.tools.tavily_search import TavilySearchResults
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage

load_dotenv()

# 1. ëª¨ë¸ ë° ë„êµ¬ ì„¤ì • (ìƒì„¸ ì˜µì…˜ ì¶”ê°€)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
search_tool = TavilySearchResults(
    max_results=3, 
    search_depth="advanced", 
    include_answer=True
)
llm_with_tools = llm.bind_tools([search_tool])

# 2. ì‹œìŠ¤í…œ í…œí”Œë¦¿ ì¶”ê°€
template = "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” assistantì…ë‹ˆë‹¤. ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•ŒëŠ” íƒ€ë¹Œë¦¬ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."

questions = [
    "2025 ë¡¤ë“œì»µ ìš°ìŠ¹ì ì´ë¦„",
    "2026ë…„ 1ì›” í˜„ì¬ ë¹„íŠ¸ì½”ì¸ ê°€ê²©", 
    "ìµœì‹  ë°œí‘œëœ íœ´ë¨¸ë…¸ì´ë“œ ë¡œë´‡ì— ëŒ€í•´ ì•Œë ¤ì¤˜"
]

print("ğŸš€ ìµœì¢… ë²„ì „ ì§ˆë¬¸ ë‹µë³€ ì‹œì‘í•©ë‹ˆë‹¤...\n")

for q in questions:
    print(f"â“ ì§ˆë¬¸: {q}")
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì§ˆë¬¸ìœ¼ë¡œ ì‹œì‘
    messages = [SystemMessage(content=template), HumanMessage(content=q)]
    
    res = llm_with_tools.invoke(messages)
    messages.append(res) # AIì˜ ì²« ë²ˆì§¸ ì‘ë‹µ ì €ì¥
    
    if res.tool_calls:
        for tool_call in res.tool_calls:
            print(f"ğŸ” '{tool_call['name']}' ê²€ìƒ‰ ì¤‘...")
            try:
                out = search_tool.invoke(tool_call["args"])
                messages.append(ToolMessage(
                    tool_call_id=tool_call["id"], 
                    content=str(out)
                ))
            except Exception as e:
                messages.append(ToolMessage(tool_call_id=tool_call["id"], content=str(e)))
        
        # ì „ì²´ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        final_res = llm.invoke(messages)
        print(f"ğŸ’¡ ë‹µë³€: {final_res.content}\n")
    else:
        print(f"ğŸ’¡ ë‹µë³€: {res.content}\n")